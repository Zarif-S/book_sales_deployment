# 3. Classical techniques:

1. Perform decomposition on the data for both books. Determine what type of decomposition is suitable for each book, and comment on the components' characteristics.
2. Perform ACF and PACF on both books. Comment on the results and what they indicate. (Optional can use ljung box too)
3. Check for stationarity (ADF) of the data for both books. Comment on the results and what they indicate.
4. Perform Auto ARIMA on both books. **The forecast horizon is the final 32 weeks of data.** All prior data (from 2012-01-01 onwards) can be used as the training data. Set reasonable bounds for Auto ARIMA's various parameters so that Auto ARIMA can identify the most suitable model.
5. Comment on the best model provided by Auto ARIMA for both books.
6. Find the residuals of the 'best' model for both books. Comment on the residuals.
7. Use the best model to predict the final 32 weeks of data. Plot the prediction along with the confidence intervals for both books.
8. Comment on how the prediction compares with the actual values.


# 1. Decomposition for Both Books
# Filter for the first ISBN (The Alchemist)
book1_data_alchemist = filtered_sales_data_2_books[filtered_sales_data_2_books['ISBN'] == '9780722532935']
# Filter for the second ISBN (The Very Hungry Caterpillar)
book2_data_caterpillar = filtered_sales_data_2_books[filtered_sales_data_2_books['ISBN'] == '9780241003008']

book1_data_alchemist

filtered_sales_data_2_books

book2_data_caterpillar

# Plot Sales Data for Both Books (Plotly)

def plot_sales_data(data, isbn, title, color):
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=data.index,
        y=data['Volume'],
        mode='lines+markers',
        name=isbn,
        line=dict(color=color)
    ))
    fig.update_layout(
        title=f'Weekly Sales Data (Volume) for {title}',
        xaxis_title='Date',
        yaxis_title='Volume',
        template='plotly_white',
        width=1100,
        height=400
    )
    fig.show()

# Plot for The Alchemist
plot_sales_data(book1_data_alchemist, '9780722532935', 'The Alchemist (Paperback)', 'black')

# Plot for The Very Hungry Caterpillar
plot_sales_data(book2_data_caterpillar, '9780241003008', 'The Very Hungry Caterpillar (Hardback)', 'blue')

## 3.1 Decomp

Seasonal variantion around christmas - (Dec 12/22 for both books) however the The Very Hungry Caterpillar also shows a second peak around March 9th.

Interestingly, there are 2 childrens book weeks in a year [reference](https://www.publishersweekly.com/pw/by-topic/childrens/childrens-industry-news/article/91547-2023-children-s-book-week-theme-and-poster-revealed.html) in 2023 this was held May 1–7 and November 6–12.

The very hungry caterpillar being a childrens book, sales start to increase around Nov 11th until December after which sales drop off after christmas.

This trend does not repeat in May so this could be coincidental with christmas sales, however there is an increase in sales in early march, further investigation is required as to why this could be.

Additive decomposition makes the most sense. It is used if the seasonal variation is relatively constant over time or the variation around the trend does not vary with the level of the time series.

# 2. Decomposition using STL
# Additive Decomposition for The Alchemist
stl_alchemist = STL(book1_data_alchemist['Volume'], period=52)
stl_results_alchemist = stl_alchemist.fit()
stl_results_alchemist.plot()
plt.suptitle('STL Additive Decomposition for The Alchemist', y=1.01)
plt.show()

# Store the STL residuals
stl_residuals_book1 = stl_results_alchemist.resid

# Additive decomposition for reasonable trend extraction
decomposition_additive = seasonal_decompose(book1_data_alchemist['Volume'], model='additive', period=52)
trend_book1_additive = decomposition_additive.trend.dropna()

# Comparison: Trend with Reasonable vs Unreasonable Parameters in STL
stl_reasonable = STL(book1_data_alchemist['Volume'], period=52, seasonal=53, trend=None)  # Reasonable
stl_unreasonable = STL(book1_data_alchemist['Volume'], period=10, seasonal=13, trend=None)  # Unreasonable

res_reasonable = stl_reasonable.fit()
res_unreasonable = stl_unreasonable.fit()

# Plot the comparison of trends
plt.figure(figsize=(14, 6))
plt.plot(book1_data_alchemist.index, book1_data_alchemist['Volume'], label='Actual Volume', color='grey', alpha=0.7)
plt.plot(book1_data_alchemist.index, res_reasonable.trend, label='Trend (STL with reasonable parameters)', color='blue')
plt.plot(trend_book1_additive.index, trend_book1_additive, label='Trend (Seasonal Additive Decomposition)', color='red')
plt.plot(book1_data_alchemist.index, res_unreasonable.trend, label='Trend (STL with unreasonable parameters)', color='orange')

# Adding labels and title
plt.legend()
plt.title('Trend Comparison for The Alchemist ISBN 9780722532935')
plt.xlabel('Date')
plt.ylabel('Volume')
plt.show()

# Conclusion: STL and seasonal decomposition perform similarly with STL slightly outperforming, but wrong period and seasonality show that the trend retains the seasonal component and the resulting trend performs poorly.

# Additive decomp for Caterpillar (should be multiplicative but this is for testing purposes)

# Decomposing the time series using STL
stl_object = STL(book2_data_caterpillar.Volume, period=52, seasonal=53,trend=None)
stl_results = stl_object.fit()

# Plotting the STL decomposition
stl_results.plot()
plt.suptitle('Decomposition for The Very Hungry Caterpillar', y=1.01)
plt.show()

# Store the STL residuals
stl_residuals_book2 = stl_results.resid

# Multiplicative Decomposition for The Very Hungry Caterpillar (with log transformation)
book2_data_caterpillar['log_volume'] = np.log(book2_data_caterpillar['Volume'].replace(0, np.nan))
stl_caterpillar = STL(book2_data_caterpillar['log_volume'].dropna(), period=52)
stl_results_caterpillar = stl_caterpillar.fit()
stl_results_caterpillar.plot()
plt.suptitle('STL Multiplicative Decomposition for The Very Hungry Caterpillar', y=1.01)
plt.show()

# Store the residuals (exponentiate to reverse log transformation)
stl_residuals_book2 = np.exp(stl_results_caterpillar.resid)

# decompose and visualize
def decompose_and_plot(df, title, period):
    # Plot original data
    df['Volume'].plot(title=f"{title} - Original Data", figsize=(10, 6))
    plt.show()

    # Additive decomposition
    additive_result = seasonal_decompose(df['Volume'], model='additive', period=period)
    additive_result.plot()
    plt.suptitle(f'{title} - Additive Decomposition', fontsize=16)
    plt.show()

    # Multiplicative decomposition
    multiplicative_result = seasonal_decompose(df['Volume'].fillna(0).clip(lower=1), model='multiplicative', period=period)
    multiplicative_result.plot()
    plt.suptitle(f'{title} - Multiplicative Decomposition', fontsize=16)
    plt.show()

    # Log transformation to check multiplicative pattern
    #df['log_value'] = np.log(df['Value'])
    #df['log_value'].plot(title=f"{title} - Log Transformed Data", figsize=(10, 6))
    #plt.show()

decompose_and_plot(book1_data_alchemist, title='Alchemist', period=52)

decompose_and_plot(book2_data_caterpillar, title='Caterpillar', period=52)

Its somewhat close for The Very Hungry Caterpillar - Multiplicative decompositon makes the most sense as the trend appears to increase with the level of the time series - upwards trend

n = 1000  # number of data points
white_noise = np.random.normal(0, 1, n)

# Plot white noise
plt.figure(figsize=(10, 6))
plt.plot(white_noise, label='White Noise')
plt.title('White Noise Time Series')
plt.xlabel('Time')
plt.ylabel('Value')
plt.legend()
plt.show()

## Covid Weeks

0 values that were introduced due to filling causing an error

zero_volume_weeks_alchemist = book1_data_alchemist[book1_data_alchemist['Volume'] == 0]

# Print the weeks with zero volume
print("Weeks with zero volume:")
print(zero_volume_weeks_alchemist[['ISBN', 'Title', 'Volume']])

zero_volume_weeks_caterpillar = book2_data_caterpillar[book2_data_caterpillar['Volume'] == 0]

# Create a figure
fig = go.Figure()

# Add the sales data for the first book
fig.add_trace(go.Scatter(
    x=book1_data_alchemist.index,
    y=book1_data_alchemist['Volume'],
    mode='lines',
    name='9780722532935 - The Alchemist',
    line=dict(color='black'),
    hoverinfo='x+y'
))

# Add the sales data for the second book
fig.add_trace(go.Scatter(
    x=book2_data_caterpillar.index,
    y=book2_data_caterpillar['Volume'],
    mode='lines',
    name='9780241003008 - The Very Hungry Caterpillar',
    line=dict(color='blue'),
    hoverinfo='x+y'
))

# Highlight the weeks with zero volume for both books
fig.add_trace(go.Scatter(
    x=zero_volume_weeks_alchemist.index,
    y=zero_volume_weeks_alchemist['Volume'],
    mode='markers',
    name='Zero Volume',
    marker=dict(color='red', size=8),
    hoverinfo='x+y'
))

# Update layout
fig.update_layout(
    title='Weekly Sales Data (Volume) for The Alchemist and The Very Hungry Caterpillar',
    xaxis_title='Date',
    yaxis_title='Volume',
    legend_title='Books',
    template='plotly_white'
)

# Show the plot
fig.show()

# 0 weeks are the same so

zero_volume_weeks = book1_data_alchemist[book1_data_alchemist['Volume'] == 0]

zero_volume_weeks.head()

0 values are covid weeks - https://www.instituteforgovernment.org.uk/sites/default/files/timeline-lockdown-web.pdf

26 March
Lockdown measures
legally come into force

15 June
Non-essential shops
reopen in England

5 November
Second national
lockdown comes
into force in
England

6 January
England enters
third national
lockdown

22 February
PM expected to
publish roadmap
for lifting the
lockdown

# Define lockdown periods (start and end dates) as Timestamps
lockdown_periods = [
    (pd.Timestamp("2020-03-26"), pd.Timestamp("2020-06-15")),  # First lockdown
    (pd.Timestamp("2020-11-05"), pd.Timestamp("2020-12-02")),  # Second lockdown
    (pd.Timestamp("2021-01-06"), pd.Timestamp("2021-02-21")),  # Third lockdown
]

# Create a new column for lockdown indication
def is_lockdown(week_date):
    for start, end in lockdown_periods:
        if start <= week_date <= end:
            return 1  # Mark as lockdown week
    return 0  # Not a lockdown week

# Apply the function to create a new column in zero_volume_weeks
zero_volume_weeks['Lockdown'] = zero_volume_weeks.index.to_series().apply(is_lockdown)

# Display the first 10 entries of the time series with index, Volume, and Lockdown status
for i in range(10):
    end_date = zero_volume_weeks.index[i]  # Get the date from the index
    volume = zero_volume_weeks['Volume'].iloc[i]  # Get the Volume value
    lockdown_status = zero_volume_weeks['Lockdown'].iloc[i]  # Get the Lockdown status
    print(f"End Date: {end_date}, Volume: {volume}, Lockdown: {lockdown_status}")

display(zero_volume_weeks)

all dates except for 5 weeks are lockdown weeks, presume the shops were still closed by choice due to short notice to get staff/not worthwhile for only 2 weeks (christmas/new year)/3 week notice period/lockdown reopening not announced by gov officially yet

## 3.2 ACF, PACF, Ljung Box Analysis

def plot_acf_pacf(data, title, lags=40, alpha=0.05):
    # Create the subplots
    fig = sp.make_subplots(rows=1, cols=2, subplot_titles=('ACF', 'PACF'), horizontal_spacing=0.2)

    # ACF plot with confidence intervals
    acf_values, acf_confint = sm.tsa.acf(data, nlags=lags, alpha=alpha)
    acf_lags = np.arange(len(acf_values))
    acf_upper_confint = acf_confint[:, 1] - acf_values
    acf_lower_confint = acf_values - acf_confint[:, 0]

    # Add ACF bars and confidence intervals
    fig.add_trace(go.Bar(x=acf_lags, y=acf_values, name='ACF'), row=1, col=1)
    fig.add_trace(go.Scatter(x=acf_lags, y=acf_values + acf_upper_confint, mode='lines', name='Upper CI', line=dict(color='rgba(255, 0, 0, 0.5)', dash='dash')), row=1, col=1)
    fig.add_trace(go.Scatter(x=acf_lags, y=acf_values - acf_lower_confint, mode='lines', name='Lower CI', line=dict(color='rgba(255, 0, 0, 0.5)', dash='dash')), row=1, col=1)

    # PACF plot with confidence intervals
    pacf_values, pacf_confint = sm.tsa.pacf(data, nlags=lags, alpha=alpha)
    pacf_lags = np.arange(len(pacf_values))
    pacf_upper_confint = pacf_confint[:, 1] - pacf_values
    pacf_lower_confint = pacf_values - pacf_confint[:, 0]

    # Add PACF bars and confidence intervals
    fig.add_trace(go.Bar(x=pacf_lags, y=pacf_values, name='PACF'), row=1, col=2)
    fig.add_trace(go.Scatter(x=pacf_lags, y=pacf_values + pacf_upper_confint, mode='lines', name='Upper CI', line=dict(color='rgba(255, 0, 0, 0.5)', dash='dash')), row=1, col=2)
    fig.add_trace(go.Scatter(x=pacf_lags, y=pacf_values - pacf_lower_confint, mode='lines', name='Lower CI', line=dict(color='rgba(255, 0, 0, 0.5)', dash='dash')), row=1, col=2)

    # Update layout
    fig.update_layout(title_text=f'ACF and PACF for {title}', height=500, width=900)
    fig.update_xaxes(title_text="Lags", row=1, col=1)
    fig.update_yaxes(title_text="ACF", row=1, col=1)
    fig.update_xaxes(title_text="Lags", row=1, col=2)
    fig.update_yaxes(title_text="PACF", row=1, col=2)

    fig.show()

def plot_acf_pacf(data, title, lags=40):
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    sm.graphics.tsa.plot_acf(data.dropna(), lags=lags, ax=axes[0], alpha=0.05)
    sm.graphics.tsa.plot_pacf(data.dropna(), lags=lags, ax=axes[1], alpha=0.05)
    plt.suptitle(f'ACF and PACF for {title}')
    plt.show()

# ACF default arguments.
# x                     : The time series object.
# lags=None             : Number of lags to calculate. If None selects automatically.
# alpha=0.05            : Confidence level to use for insignificance region.
# adjusted=False        : Related to calculation method.
# fft=False             : Related to calculation method (fast fourier transform).
# missing='none'        : How to treat missing values.
# zero=True             : Return 0-lag autocorrelation?
# bartlett_confint=True : Related to calculation of insignificance region.

# ACF & PACF for The Alchemist
plot_acf_pacf(book1_data_alchemist['Volume'], 'The Alchemist')

ACF shows strong autocorrelation for both books that decays slowly as the lags increase - statistically significant, there also seems to be some seasonality present. No sharp cutoff in the ACF so cannot determine MA (Q) order easily

# ACF & PACF for The Very Hungry Caterpillar
plot_acf_pacf(book2_data_caterpillar['Volume'], 'The Very Hungry Caterpillar')

PACF(0,1) shows strong correlation just like ACF(0,1). All  lags greater than 1 are eliminated in the PACF however except for 2 values that are just about significant for The Very Hungry Caterpillar, The Alchemist is also quite borderline.

PACF for both books shows us that the direct correlations between the time series and its lagged values are primarily present for the first lag.

There is also some kind of seasonality present as there are repeated patterns, this is also present for the first book, The alcehmist, however it is less pronounced.

Seasonal AR order (P) in my data looks like 1 for both

When plotting ACF/PACF best to use double the seaonality period to better see the seasonal trend

# ACF & PACF for both books
plot_acf_pacf(book1_data_alchemist['Volume'], 'The Alchemist', lags=104)
plot_acf_pacf(book2_data_caterpillar['Volume'], 'The Very Hungry Caterpillar', lags=104)

# Calculate ACF and confidence intervals
acf_coef = acf(book1_data_alchemist['Volume'], alpha=0.05)

# List to hold significant lags
sig_acf = []
for i in range(1, len(acf_coef[0])):
    # Check for significant positive autocorrelation
    if acf_coef[0][i] > (acf_coef[1][i][1] - acf_coef[0][i]):
        sig_acf.append(i)
    # Check for significant negative autocorrelation
    elif acf_coef[0][i] < (acf_coef[1][i][0] - acf_coef[0][i]):
        sig_acf.append(i)

print("Significant lags acf:", sig_acf)

Since I have significant lags up to lag 11 for the alchemist, this might suggest trying an AR model with an order close to 11 **(for q)**, where previous values up to 11 weeks are used to predict the next week’s volume, or 28 in the case of caterpillar, starting with a low q value and increasing complexity to reduce overfitting and maximise generalisability

# Calculate ACF and confidence intervals
acf_coef = acf(book2_data_caterpillar['Volume'], alpha=0.05)

# List to hold significant lags
sig_acf = []
for i in range(1, len(acf_coef[0])):
    # Check for significant positive autocorrelation
    if acf_coef[0][i] > (acf_coef[1][i][1] - acf_coef[0][i]):
        sig_acf.append(i)
    # Check for significant negative autocorrelation
    elif acf_coef[0][i] < (acf_coef[1][i][0] - acf_coef[0][i]):
        sig_acf.append(i)

print("Significant lags:", sig_acf)

#Significant lags:
pacf_coef = pacf(book1_data_alchemist['Volume'], alpha=.05)
sig_pacf = []
for i in range(1, len(pacf_coef[0])):
    if pacf_coef[0][i] > (pacf_coef[1][i][1] - pacf_coef[0][i]):
        sig_pacf.append(i)
    elif pacf_coef[0][i] < (pacf_coef[1][i][0] - pacf_coef[0][i]):
        sig_pacf.append(i)

sig_pacf

Since I have significant lags up to lag 1 for the alchemist, this might suggest trying an MA model with an order close to 1 **(for p)**, where previous values up to 1 weeks are used to predict the next week’s volume, or up to a value of 22 for p in the case of caterpillar, starting with a low p value and increasing complexity to reduce overfitting and maximise generalisability

#Significant lags:
pacf_coef = pacf(book2_data_caterpillar['Volume'], alpha=.05)
sig_pacf = []
for i in range(1, len(pacf_coef[0])):
    if pacf_coef[0][i] > (pacf_coef[1][i][1] - pacf_coef[0][i]):
        sig_pacf.append(i)
    elif pacf_coef[0][i] < (pacf_coef[1][i][0] - pacf_coef[0][i]):
        sig_pacf.append(i)

sig_pacf

# Ljung-Box Test for Multiple Lags (1 to 10)
def perform_ljung_box_test(data, label, max_lag):
    lb_test = acorr_ljungbox(data.dropna(), lags=list(range(1, max_lag + 1)), return_df=True)
    print(f"Ljung-Box test output for {label} (Lags 1 to {max_lag}):")
    display(lb_test)  # Using display to print DataFrame clearly

# Apply the Ljung-Box test for lags 1 to 10 for both books
perform_ljung_box_test(book1_data_alchemist['Volume'], 'The Alchemist', 10)
perform_ljung_box_test(book2_data_caterpillar['Volume'], 'The Very Hungry Caterpillar', 10)

Significant Autocorrelation: Both time series data show substantial evidence of autocorrelation at all lags tested, with increasingly large test statistics and very small p-values.

Model Consideration: The presence of significant autocorrelation suggests that standard forecasting methods may be insufficient. Time series models that incorporate this autocorrelation, such as ARIMA should be utilised.

## 3.3 Stationarity Tests (ADF) - review text

****Augmented Dickey-Fuller (ADF) Test****

Focuses on identifying whether there is evidence that the series is non-stationary due to a unit root.

2 options:

If p-value < 0.05, reject the null hypothesis (stationary).

Test statistic vs. critical values: If the test statistic is less than the critical value (at a given level, e.g., 5%), reject the null hypothesis (stationary).

def adf_test(series, label, alpha=0.05):
    result = adfuller(series.dropna(), autolag='BIC')  # autolag='BIC' for optimal lag selection
    adf_stat, p_value, critical_values = result[0], result[1], result[4]

    # Print results
    print(f"\nADF Test for {label}:")
    print(f"ADF Test Statistic: {adf_stat}")
    print(f"p-value: {p_value}")
    print(f"Critical Values: {critical_values}")

    # Decision based on p-value and critical values
    if p_value < alpha:
        print(f"{label}: Reject null hypothesis (series is stationary), p value is less than significant level")
    else:
        print(f"{label}: Fail to reject null hypothesis (series is non-stationary)")

    # Additional decision based on test statistic and critical value (optional)
    if adf_stat < critical_values['5%']:
        print(f"{label}: Test statistic is less than 5% critical value, confirming stationarity")
    else:
        print(f"{label}: Test statistic is greater than 5% critical value, confirming non-stationarity")

# Example usage for both books:
adf_test(book1_data_alchemist['Volume'], 'The Alchemist')
adf_test(book2_data_caterpillar['Volume'], 'The Very Hungry Caterpillar')

**Kwiatkowski-Phillips-Schmidt-Shin (KPSS) Test**

Focuses on determining whether the series is stationary around a deterministic trend or level.

def kpss_test(series, label, alpha=0.05):
    kpss_stat, kpss_pvalue, kpss_lags, kpss_crit = kpss(series.dropna(), regression='c', nlags='auto')

    # Print KPSS results
    print(f"\nKPSS Test for {label}:")
    print(f"KPSS Test Statistic: {kpss_stat}")
    print(f"p-value: {kpss_pvalue}")
    print(f"Critical Values: {kpss_crit}")
    print(f"Lags Used: {kpss_lags}")

    # Decision based on p-value
    if kpss_pvalue < alpha:
        print(f"{label}: Reject null hypothesis (series is non-stationary based on p-value)")
    else:
        print(f"{label}: Fail to reject null hypothesis (series is stationary based on p-value)")

    # Additional decision based on test statistic
    if kpss_stat > kpss_crit['5%']:
        print(f"{label}: Test statistic is greater than 5% critical value, confirming non-stationarity")
    else:
        print(f"{label}: Test statistic is less than 5% critical value, confirming stationarity")

# Example usage for both books:
kpss_test(book1_data_alchemist['Volume'], 'The Alchemist')
kpss_test(book2_data_caterpillar['Volume'], 'The Very Hungry Caterpillar')

Different Perspectives:

* The ADF and KPSS tests have opposite null hypotheses. A series might fail to reject the null hypothesis of non-stationarity in the ADF test but also fail to reject the null hypothesis of stationarity in the KPSS test.
* This contradictory result often happens when the series is borderline or marginally stationary/non-stationary.

Testing Both Unit Root and Trend Stationarity:

* Using the KPSS test provides insight into whether a series is trend-stationary rather than strictly non-stationary due to a unit root. In some cases, data may exhibit stationarity around a trend rather than having a stochastic trend (unit root).
* The KPSS test checks for stationarity around a trend, making it suitable when the series has a consistent pattern or deterministic drift.

Complementary Tests:

* If ADF rejects the null (stationary) and KPSS fails to reject the null (also stationary): Strong evidence of stationarity.
* If ADF fails to reject the null (non-stationary) and KPSS rejects the null (non-stationary): Strong evidence of non-stationarity.
* Mixed results suggest more nuanced stationarity, often indicating the presence of deterministic trends or structural breaks.

Detecting Trend Stationarity:

* The KPSS test is particularly useful when you suspect that the series is stationary around a deterministic trend rather than being purely random with a stochastic trend.

Understanding the Nature of Non-Stationarity:

* Using the KPSS test can help determine if non-stationarity is due to a unit root (random walk behavior) or simply a deterministic trend that can be removed with differencing or detrending.

**Phillips-Perron (PP) Test**

When dealing with data that may have complex error structures, varying volatility (heteroscedasticity), or when you want a simpler, non-parametric adjustment for autocorrelation.

def pp_test(series, label, alpha=0.05):
    pp_result = PhillipsPerron(series.dropna())

    # Extracting the test statistic, p-value, and critical values
    pp_stat = pp_result.stat
    pp_pvalue = pp_result.pvalue
    pp_critical_values = pp_result.critical_values

    # Print PP test results
    print(f"\nPhillips-Perron Test for {label}:")
    print(f"Phillips-Perron Test Statistic: {pp_stat}")
    print(f"Phillips-Perron p-value: {pp_pvalue}")
    print(f"Phillips-Perron Critical Values: {pp_critical_values}")

    # Decision based on p-value
    if pp_pvalue < alpha:
        print(f"{label}: Reject null hypothesis (series is stationary based on p-value)")
    else:
        print(f"{label}: Fail to reject null hypothesis (series is non-stationary based on p-value)")

    # Additional decision based on test statistic
    if pp_stat < pp_critical_values['5%']:
        print(f"{label}: Test statistic is less than 5% critical value, confirming stationarity")
    else:
        print(f"{label}: Test statistic is greater than 5% critical value, confirming non-stationarity")

pp_test(book1_data_alchemist['Volume'], 'The Alchemist - Phillips-Perron Test')
pp_test(book2_data_caterpillar['Volume'], 'The Very Hungry Caterpillar - Phillips-Perron Test')

Robustness to Heteroscedasticity and Serial Correlation:

* The PP test adjusts the test statistic for serial correlation and heteroscedasticity without adding lagged terms, making it less sensitive to model specification errors that can arise in the ADF test.
* This robustness makes the PP test particularly useful when dealing with financial data or macroeconomic series that often exhibit varying volatility over time.

Simpler Adjustment Mechanism:

* Since the PP test does not require selecting lags, it simplifies the testing process and avoids potential issues from incorrect lag length selection, which can sometimes mislead ADF test results.

Consistent Results Across Different Error Structures:

* The PP test tends to be more consistent in situations where the data exhibit complex error structures, such as clustered volatility or unknown autocorrelation, which are not adequately addressed by simply adding lagged terms.

The PP test is computationally simpler to run as it directly adjusts the Dickey-Fuller statistic without needing an iterative process of adding lags.

**Zivot-Andrews Test (with a structural break)**

This test explicitly tests for unit roots while allowing for a single structural break in the series. It endogenously determines the most likely point of the break within the series, adjusting the test statistic accordingly.

def za_test(series, label, alpha=0.05):
    # Ensure the series is cast to float to avoid type errors
    series = series.astype('float64').dropna()

    # Run the Zivot-Andrews test
    za_result = ZivotAndrews(series)

    # Extracting the test statistic, p-value, and critical values
    za_stat = za_result.stat
    za_pvalue = za_result.pvalue
    za_critical_values = za_result.critical_values

    # Print ZA test results
    print(f"\nZivot-Andrews Test for {label}:")
    print(f"Zivot-Andrews Test Statistic: {za_stat}")
    print(f"Zivot-Andrews p-value: {za_pvalue}")
    print(f"Zivot-Andrews Critical Values: {za_critical_values}")

    # Decision based on p-value
    if za_pvalue < alpha:
        print(f"{label}: Reject null hypothesis (series is stationary with a structural break based on p-value)")
    else:
        print(f"{label}: Fail to reject null hypothesis (series is non-stationary based on p-value)")

    # Additional decision based on test statistic
    if za_stat < za_critical_values['5%']:
        print(f"{label}: Test statistic is less than 5% critical value, confirming stationarity with a structural break")
    else:
        print(f"{label}: Test statistic is greater than 5% critical value, confirming non-stationarity")

za_test(book1_data_alchemist['Volume'], 'The Alchemist - Zivot-Andrews Test')
za_test(book2_data_caterpillar['Volume'], 'The Very Hungry Caterpillar - Zivot-Andrews Test')

Why Use the Zivot-Andrews Test Instead of the ADF Test?

Presence of Structural Breaks:

* When data is suspected to have undergone sudden changes due to external shocks, policy changes, economic crises, or other events, the Zivot-Andrews test can accurately adjust for these breaks, which the ADF test cannot handle.
* For example, financial data affected by market crashes, or economic data impacted by policy shifts, are prime candidates for the Zivot-Andrews test.

More Accurate Stationarity Determination:

* Structural breaks can cause the ADF test to mistakenly identify a series as non-stationary.
* The Zivot-Andrews test corrects for this by adjusting for breaks, providing a more accurate reflection of whether the series is truly stationary.

Avoiding False Conclusions:

* The Zivot-Andrews test helps avoid the common pitfall where the ADF test incorrectly concludes that a series is non-stationary due to unaccounted breaks, which can lead to inappropriate modeling choices in time series forecasting.

Endogenous Break Selection:

* Unlike tests that require a user-specified break date, the Zivot-Andrews test endogenously determines the most likely break point, reducing bias and increasing the reliability of the test results.

## Summary

ADF confirmed time series is stationary.

An AR model is appropriate for a time series if the data's partial autocorrelation function (PACF) has an abrupt cut-off at a certain lag. Looking at the residual of the decomp model, we can see this is also not true, instead there is a gradual decay.

An MA model is preferred if the autocorrelation function (ACF) has an abrupt cut-off. In this case this is not true.

Considering none of the above conditions are true, then we must use an ARMA model/ARIMA

Since the p-value is significantly lower than common significance levels (e.g., 0.05 or 0.01), we can reject the null hypothesis of the ADF test.

This indicates that the time series data for The Alchemist and The very Hungry Caterpillar exhibit stationarity.

Classical modelling methods e.g ARIMA, SARIMA assume stationarity so these can be implemented without issue, differencing is not required however can experiement with d=1 too.